[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "Tutorials/Data-access.html",
    "href": "Tutorials/Data-access.html",
    "title": "4. Data access: Social media API",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nExplain how APIs enable researchers to collect data from social media platforms\nRetrieve Reddit data using RedditExtractoR, including posts, threads, and comments\nAuthenticate and collect data from the YouTube API using the tuber package\nFilter and organize retrieved data by keyword, channel, and time frame\nSave and manage collected data for later cleaning and analysis\nAutomate tasks in R: For() loops and apply() functions",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#learning-goals",
    "href": "Tutorials/Data-access.html#learning-goals",
    "title": "4. Data access: Social media API",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nExplain how APIs enable researchers to collect data from social media platforms\nRetrieve Reddit data using RedditExtractoR, including posts, threads, and comments\nAuthenticate and collect data from the YouTube API using the tuber package\nFilter and organize retrieved data by keyword, channel, and time frame\nSave and manage collected data for later cleaning and analysis\nAutomate tasks in R: For() loops and apply() functions",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#using-redditextractor-wrapper-for-the-reddit-api",
    "href": "Tutorials/Data-access.html#using-redditextractor-wrapper-for-the-reddit-api",
    "title": "4. Data access: Social media API",
    "section": "1.1 Using RedditExtractoR (Wrapper for the Reddit API)",
    "text": "1.1 Using RedditExtractoR (Wrapper for the Reddit API)\nThe RedditExtractoR package allows collection of posts, comments, and metadata from Reddit.\nCRAN documentation:\nhttps://cran.r-project.org/web/packages/RedditExtractoR/RedditExtractoR.pdf\nImportant limitation:\nMost queries return roughly up to 1,000 posts per subreddit or search. This works best for recent or popular content rather than full historical archives.\n\n# install.packages(\"RedditExtractoR\")\nlibrary(RedditExtractoR)\n\n# View available functions\nls(\"package:RedditExtractoR\")\n\n\nStep 1: Find Relevant Subreddits\n\nsubreddits &lt;- find_subreddits(\"washingtondc\")\nhead(subreddits)\n\nThis returns a data frame with subreddit names, descriptions, and subscriber counts.\n\n\nStep 2: Retrieve Thread URLs\n\nthread_urls &lt;- find_thread_urls(\n  subreddit = \"washingtondc\",\n  sort_by = \"top\",\n  period = \"week\"\n)\n\nhead(thread_urls)\nnrow(thread_urls)\n\nSearch by keyword:\n\nweekly_dc_snow &lt;- find_thread_urls(\n  keywords = \"snow\",\n  sort_by = \"top\",\n  subreddit = \"washingtondc\",\n  period = \"week\"\n)\n\nhead(weekly_dc_snow)\nnrow(weekly_dc_snow)\n\nwrite.csv(weekly_dc_snow, \"weekly_DCsnow_reddit.csv\", row.names = FALSE)\n\nMultiple keywords:\n\nkeywords &lt;- c(\"snow\", \"blizzard\", \"ice\")\n\nweekly_dc_snow_ver2 &lt;- do.call(\n  rbind,\n  lapply(keywords, function(k) {\n    find_thread_urls(\n      keywords = k,\n      sort_by = \"top\",\n      subreddit = \"washingtondc\",\n      period = \"week\"\n    )\n  })\n)\n\nnrow(weekly_dc_snow_ver2)\n\n\n\nStep 3: Extract Post and Comment Content\n\nthread_content &lt;- get_thread_content(thread_urls$url[1])\nhead(thread_content)\n\nwrite.csv(thread_content, \"DC_thread_content.csv\", row.names = FALSE)\n\n\n\nStep 4: Retrieve Data from a Specific User\n\nuser_content &lt;- get_user_content(\"enterusernamehere\")\nhead(user_content)\n\n\n\n\nFiltering by Date\n\nlibrary(lubridate)\nlibrary(dplyr)\n\nthread_urls$date &lt;- as_datetime(thread_urls$timestamp)\n\nfiltered_threads &lt;- thread_urls %&gt;%\n  filter(date &gt;= as_datetime(\"2024-03-01\") &\n         date &lt;= as_datetime(\"2024-03-10\"))",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#using-the-reddit-api-directly",
    "href": "Tutorials/Data-access.html#using-the-reddit-api-directly",
    "title": "4. Data access: Social media API",
    "section": "1.2 Using the Reddit API Directly",
    "text": "1.2 Using the Reddit API Directly\nFor more control, use Reddit’s official API.\nYou must create an app and authenticate using:\n\nclient ID\n\nclient secret\n\nusername and password\n\nDocumentation:\nhttps://support.reddithelp.com/hc/en-us/articles/16160319875092-Reddit-Data-API-Wiki\nRate limit: ~60 requests per minute for personal use.\nSteps:\n\nGo to http://reddit.com/prefs/apps\n\nClick “Create App”\n\nSelect type: Script\n\nRedirect URI: http://localhost:1410\n\nSave client ID and client secret",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#install-and-authenticate",
    "href": "Tutorials/Data-access.html#install-and-authenticate",
    "title": "4. Data access: Social media API",
    "section": "Install and Authenticate",
    "text": "Install and Authenticate\n\nlibrary(devtools)\ndevtools::install_github(\"soodoku/tuber\", build_vignettes = TRUE)\n\nlibrary(tuber)\noptions(scipen = 999)\n\nclient_ID &lt;- \"YOUR_CLIENT_ID\"\nclient_secret &lt;- \"YOUR_CLIENT_SECRET\"\n\nyt_oauth(client_ID, client_secret, token = \"\")\n\nAfter authentication, return to R when prompted.",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#retrieve-channel-information",
    "href": "Tutorials/Data-access.html#retrieve-channel-information",
    "title": "4. Data access: Social media API",
    "section": "Retrieve Channel Information",
    "text": "Retrieve Channel Information\n\nchannel_resources &lt;- get_channel_stats(\n  channel_id = \"UCBi2mrWuNuyYy4gbM6fU18Q\"\n) |&gt; as.data.frame()\n\nchannel_resources\n\nTo find a channel ID:\nYouTube → Channel → About → Share → Copy channel ID",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#search-videos-by-keyword-and-date",
    "href": "Tutorials/Data-access.html#search-videos-by-keyword-and-date",
    "title": "4. Data access: Social media API",
    "section": "Search Videos by Keyword and Date",
    "text": "Search Videos by Keyword and Date\n\nprotest_abcnews &lt;- yt_search(\n  \"protest\",\n  channel_id = \"UCBi2mrWuNuyYy4gbM6fU18Q\",\n  published_after = \"2024-12-01T00:00:00Z\",\n  published_before = \"2025-02-01T00:00:00Z\",\n  max_results = 10\n)\n\nnrow(protest_abcnews)\nhead(protest_abcnews)\n\nwrite.csv(protest_abcnews, \"protest_abcnews.csv\", row.names = FALSE)",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#retrieve-video-details-and-comments",
    "href": "Tutorials/Data-access.html#retrieve-video-details-and-comments",
    "title": "4. Data access: Social media API",
    "section": "Retrieve Video Details and Comments",
    "text": "Retrieve Video Details and Comments\n\ndetails &lt;- get_video_details(video_id = \"Yvb6ko7ZWaI\") |&gt; as.data.frame()\nstats &lt;- get_stats(video_id = \"Yvb6ko7ZWaI\") |&gt; as.data.frame()\n\nABC_comments &lt;- get_comment_threads(\n  filter = c(video_id = \"Yvb6ko7ZWaI\"),\n  max_results = 20\n)\n\nhead(ABC_comments)\n\nOptional table display:\n\nlibrary(knitr)\nkable(head(ABC_comments, 10))",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#retrieve-captions",
    "href": "Tutorials/Data-access.html#retrieve-captions",
    "title": "4. Data access: Social media API",
    "section": "Retrieve Captions",
    "text": "Retrieve Captions\n\n# remotes::install_github(\"jooyoungseo/youtubecaption\")\nlibrary(youtubecaption)\n\nurl &lt;- \"https://www.youtube.com/watch?v=Yvb6ko7ZWaI\"\ncaptions &lt;- get_caption(url)\n\nhead(captions)",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#lapply-function",
    "href": "Tutorials/Data-access.html#lapply-function",
    "title": "4. Data access: Social media API",
    "section": "lapply() Function",
    "text": "lapply() Function\nApplies a function to each element of a vector or list.\n\nnumbers &lt;- c(4, 9, 16)\nlapply(numbers, sqrt)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 3\n\n[[3]]\n[1] 4\n\nwords &lt;- c(\"snow\", \"rain\", \"sun\")\nlapply(words, toupper)\n\n[[1]]\n[1] \"SNOW\"\n\n[[2]]\n[1] \"RAIN\"\n\n[[3]]\n[1] \"SUN\"",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#looping-over-reddit-threads",
    "href": "Tutorials/Data-access.html#looping-over-reddit-threads",
    "title": "4. Data access: Social media API",
    "section": "Looping Over Reddit Threads",
    "text": "Looping Over Reddit Threads\n\nthreads &lt;- find_thread_urls(\n  subreddit = \"washingtondc\",\n  sort_by = \"top\",\n  period = \"week\"\n)\n\nurls &lt;- threads$url[1:3]\n\nthread_contents &lt;- lapply(urls, function(u) {\n  get_thread_content(u)\n})\n\nthread_contents[[1]]\n\nUsing a for loop:\n\nthread_contents &lt;- list()\n\nfor (i in 1:3) {\n  thread_contents[[i]] &lt;- get_thread_content(threads$url[i])\n}",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-access.html#summary",
    "href": "Tutorials/Data-access.html#summary",
    "title": "4. Data access: Social media API",
    "section": "Summary:",
    "text": "Summary:\nThis module introduced practical techniques for collecting digital data directly from online platforms. Because APIs and platform policies change frequently, always verify current documentation before starting a project.",
    "crumbs": [
      "Tutorials",
      "4. Data access: Social media API"
    ]
  },
  {
    "objectID": "Tutorials/Data-structure-and-cleaning.html",
    "href": "Tutorials/Data-structure-and-cleaning.html",
    "title": "2. Data structure & cleaning",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nUnderstand vectors, data types, and data frames in R\nConvert between different data types\nUse the tidyverse to clean and summarize data\nApply pipe operators to write readable R code",
    "crumbs": [
      "Tutorials",
      "2. Data structure & cleaning"
    ]
  },
  {
    "objectID": "Tutorials/Data-structure-and-cleaning.html#learning-goals",
    "href": "Tutorials/Data-structure-and-cleaning.html#learning-goals",
    "title": "2. Data structure & cleaning",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nUnderstand vectors, data types, and data frames in R\nConvert between different data types\nUse the tidyverse to clean and summarize data\nApply pipe operators to write readable R code",
    "crumbs": [
      "Tutorials",
      "2. Data structure & cleaning"
    ]
  },
  {
    "objectID": "Tutorials/Data-structure-and-cleaning.html#introduction-to-tidyverse-and-the-mtcars-dataset",
    "href": "Tutorials/Data-structure-and-cleaning.html#introduction-to-tidyverse-and-the-mtcars-dataset",
    "title": "2. Data structure & cleaning",
    "section": "2. Introduction to tidyverse and the mtcars dataset",
    "text": "2. Introduction to tidyverse and the mtcars dataset\nIn this part of the module, we will now use mtcars, a built-in dataset from Motor Trend magazine (1974), to learn basic data cleaning and preprocessing.\n\n\n\n\n\n\nNote\n\n\n\nWhen working with large datasets, filtering variables of interest, summarizing data, arranging, and mutating (such as to create new columns after averaging the scores) can significantly save time and memory usage in your software. We will use the ‘tidyverse’ package to do so. Tidyverse will automatically load packages that you’re likely to use in everyday data analyses, such as dplyr, readr, tidyr … etc.",
    "crumbs": [
      "Tutorials",
      "2. Data structure & cleaning"
    ]
  },
  {
    "objectID": "Tutorials/Data-structure-and-cleaning.html#summary",
    "href": "Tutorials/Data-structure-and-cleaning.html#summary",
    "title": "2. Data structure & cleaning",
    "section": "Summary:",
    "text": "Summary:\nIn this chapter, you learned how to work with core R structures such as vectors, data frames, objects, lists, and matrices. You also practiced converting between data types, using pipe operators to write clear and readable code, and cleaning and summarizing real-world datasets. These foundational skills prepare you to move from data preparation to analysis. In the next chapter, we will build on these concepts to explore data visualization and interpretation.",
    "crumbs": [
      "Tutorials",
      "2. Data structure & cleaning"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee @knuth84 for additional discussion of literate programming."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Computational Social Science with R",
    "section": "",
    "text": "About This Textbook\nThis textbook introduces practical and beginner-friendly computational approaches to social science research. It consists of hands-on tutorials designed to help you work with real-world data, formulate meaningful research questions, and explore social phenomena using computational methods.\nThe course (CCMS787-01) emphasizes learning by doing. You will collect, clean, analyze, and visualize data using R, while engaging with conceptual foundations and ethical considerations that guide responsible data-driven research.\nComputational methods are powerful, but they are not neutral. Throughout the tutorials, we will think critically about what data represent, what they exclude, and how analytical choices shape conclusions.\nBy the end of the tutorials, you should feel confident designing data-driven research and continuing to learn new methods independently. You will likely also find yourself asking better questions. That part is intentional.\n\n“Finding the right question is often more important than finding the answer.”\n- John Tukey\n\n\n\nAbout the Author\nI am Gyo “Hyun” Koo, a researcher and educator focused on how computational tools can help us understand the impact of digital platforms on public sociopolitical behaviors. My approach emphasizes learning by doing and the ethical use of diverse datasets. To learn more about my research and teaching, visit my personal website\n\n\n\n\n\n\nTip\n\n\n\nHow to use this site\n\nRead tutorials in order\nCopy and run code in RStudio\nPractice questions with examples\nCCMS787-01 students: Ask questions when something feels unclear"
  },
  {
    "objectID": "Tutorials/Intro-to-R.html",
    "href": "Tutorials/Intro-to-R.html",
    "title": "1. Introduction to R",
    "section": "",
    "text": "In this first tutorial, we begin with the core building blocks of R. We will explore data structures in more detail in Chapter 2.\n# --------------------------------------------------\n# Creating objects and basic arithmetic\n# --------------------------------------------------\n\n# Store values using the assignment operator &lt;-\nx &lt;- 10\ny &lt;- 5\n\n# Perform basic calculations\nx + y\n\n[1] 15\n\nx * y\n\n[1] 50\n\n# --------------------------------------------------\n# Creating and exploring a data frame\n# --------------------------------------------------\n\n# Create a simple data frame of cities\ncities &lt;- data.frame(\n  name = c(\"Washington DC\", \"Paris\", \"Seoul\"),\n  population = c(693645, 2048472, 9600000)\n)\n\n# View the data frame\ncities\n\n           name population\n1 Washington DC     693645\n2         Paris    2048472\n3         Seoul    9600000\n\n# Check column names\ncolnames(cities)\n\n[1] \"name\"       \"population\"\n\n# --------------------------------------------------\n# Accessing data\n# --------------------------------------------------\n\n# Access a column using $\ncities$population\n\n[1]  693645 2048472 9600000\n\n# Access rows and columns using indexing [row, column]\ncities[1, ]          # First row\n\n           name population\n1 Washington DC     693645\n\ncities[, \"name\"]     # Name column\n\n[1] \"Washington DC\" \"Paris\"         \"Seoul\"        \n\n# --------------------------------------------------\n# Using functions\n# --------------------------------------------------\n\n# Calculate summary statistics\nmean(cities$population)\n\n[1] 4114039\n\nsd(cities$population)\n\n[1] 4799033\n\n# --------------------------------------------------\n# Creating new variables\n# --------------------------------------------------\n\n# Add a new column\ncities$double_population &lt;- cities$population * 2\n\n# View updated data frame\ncities\n\n           name population double_population\n1 Washington DC     693645           1387290\n2         Paris    2048472           4096944\n3         Seoul    9600000          19200000",
    "crumbs": [
      "Tutorials",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "Tutorials/Intro-to-R.html#your-first-code-example",
    "href": "Tutorials/Intro-to-R.html#your-first-code-example",
    "title": "1. Introduction to R",
    "section": "Your first code example",
    "text": "Your first code example\nAs a warm-up, we begin with a cowsay package to run our initial codes.\n\n\n\n\n\n\nNote\n\n\n\nThe cowsay package displays messages using ASCII art animals. This makes console output more fun and engaging.\n\n\nInstall the package from CRAN onto your computer and load the package (once you install a package, you do not need to re-install it again. Next time, just use library() without install.packages().\n\n#install.packages(\"cowsay\")  \nlibrary(cowsay)\nsay(\"Hello world!\")\n\n\n -------------- \n&lt; Hello world! &gt;\n -------------- \n      \\\n       \\\n\n        ^__^ \n        (oo)\\ ________ \n        (__)\\         )\\ /\\ \n             ||------w|\n             ||      ||\n\nsay(\"I'm EGGS-hausted!.\\n - Fluffy chicken\",\n    by = \"chicken\")\n\n\n -------------------------------------- \n&lt; I'm EGGS-hausted!.  - Fluffy chicken &gt;\n -------------------------------------- \n      \\\n       \\\n         _\n       _/ }\n      `&gt;' \\\n      `|   \\\n       |   /'-.     .-.\n        \\'     ';`--' .'\n         \\'.    `'-./\n          '.`-..-;`\n            `;-..'\n            _| _|\n            /` /` [nosig]\n \n\n\n\n\n\n\n\n\nTip\n\n\n\nTip: If you type ? followed by a function name, the Help window will open on the right and explain what the function does\n\n\n\n?say\n\n\n\n\n\n\n\nNote\n\n\n\nTry different animals! If you enter an animal name that is not supported, R will return an error such as: Error: unknown animal. The message will also display a list of supported animals; for example frog, ghost, pumpkin, dragon, bat, owl, duck, yoda, alligator.\n\n\n\nsay(\"catfact\", \"cat\")\n\n\n --------------------------------------------------------- \n/ Cats hate the water because their fur does not insulate \\\n|well when it’s wet. The Turkish Van, however, is one   |\n| cat that likes swimming. Bred in central Asia, its coat |\n\\ has a unique texture that makes it water resistant.     /\n --------------------------------------------------------- \n         \\\n          \\\n\n            |\\___/|\n          ==) ^Y^ (==\n            \\  ^  /\n             )=*=(\n            /     \\\n            |     |\n           /| | | |\\\n           \\| | |_|/\\\n      jgs  //_// ___/\n               \\_)\n  \n\nsay(\"It is going to snow this weekend. Stay warm! \n    — Dr. Koo\", \"snowman\")\n\n\n --------------------------------------------------------- \n&lt;It is going to snow this weekend. Stay warm!  — Dr. Koo&gt;\n --------------------------------------------------------- \n  \\\n   \\\n\n     _[_]_\n      (\")\n  &gt;--( : )--&lt;\n    (__:__) [nosig]",
    "crumbs": [
      "Tutorials",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "Tutorials/Intro-to-R.html#practice-years-since-major-historical-events",
    "href": "Tutorials/Intro-to-R.html#practice-years-since-major-historical-events",
    "title": "1. Introduction to R",
    "section": "Practice: Years Since Major Historical Events",
    "text": "Practice: Years Since Major Historical Events\n\n\n\n\n\n\nTip\n\n\n\nTask\n\nCalculate how many years have passed since: the first computer (1946) + the first moon landing (1969).\nPrint each result and the total number of years.\n\n\n\n\nfirst_computer &lt;- 1946\nfirst_moon_landing &lt;- 1969\ncurrent_year &lt;- 2026 #ENTER THE CURRENT YEAR\n\n\nCalculate years since events\n\nyears_since_computer &lt;- current_year - first_computer\nprint(years_since_computer)\n\n[1] 80\n\nyears_since_moon &lt;- current_year - first_moon_landing\nprint(years_since_moon)\n\n[1] 57\n\n\n\n\nSum of years since events\n\ntotal_years &lt;- years_since_computer + years_since_moon\nprint(total_years)\n\n[1] 137\n\n\n\n\n\n\n\n\nNote\n\n\n\nTip: Remember to label your R code chunks\nWhen writing code in your R Markdown, make sure that every R code block starts with three backticks and {r}, and also ends with three backticks. The backtick key is usually located next to the 1 key on your keyboard (the same key as ~).",
    "crumbs": [
      "Tutorials",
      "1. Introduction to R"
    ]
  },
  {
    "objectID": "Tutorials/index.html",
    "href": "Tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Introduction to R\nData structure and cleaning\nData pre-processing and visualization\nData accss: Social media API",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "Tutorials/Data-preprocessing.html",
    "href": "Tutorials/Data-preprocessing.html",
    "title": "3. Data pre-processing and visualization",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nInspect and prepare datasets for analysis by checking structure and formats\nClean and pre-process data by creating variables and standardizing dates\nMerge datasets using shared keys such as state and date\nCreate basic visualizations using ggplot2, including line graphs and bar charts\nCustomize plots with titles, labels, and themes\nExport and save visualizations for reports or presentations\n\n\nIn the previous module, we explored data types and why they matter. R does not always import variables in the format you expect. For example, numbers may be read as character strings. When this happens, you must convert (coerce) the variable to the correct type before analysis and visualization.",
    "crumbs": [
      "Tutorials",
      "3. Data pre-processing and visualization"
    ]
  },
  {
    "objectID": "Tutorials/Data-preprocessing.html#learning-goals",
    "href": "Tutorials/Data-preprocessing.html#learning-goals",
    "title": "3. Data pre-processing and visualization",
    "section": "",
    "text": "By the end of this tutorial, you will be able to:\n\nInspect and prepare datasets for analysis by checking structure and formats\nClean and pre-process data by creating variables and standardizing dates\nMerge datasets using shared keys such as state and date\nCreate basic visualizations using ggplot2, including line graphs and bar charts\nCustomize plots with titles, labels, and themes\nExport and save visualizations for reports or presentations\n\n\nIn the previous module, we explored data types and why they matter. R does not always import variables in the format you expect. For example, numbers may be read as character strings. When this happens, you must convert (coerce) the variable to the correct type before analysis and visualization.",
    "crumbs": [
      "Tutorials",
      "3. Data pre-processing and visualization"
    ]
  },
  {
    "objectID": "Tutorials/Data-preprocessing.html#why-date-formats-matter",
    "href": "Tutorials/Data-preprocessing.html#why-date-formats-matter",
    "title": "3. Data pre-processing and visualization",
    "section": "Why date formats matter",
    "text": "Why date formats matter\nBoth datasets include time, but they record it differently. The COVID dataset uses a week index, while the mobility dataset uses a calendar date. Before merging, we must standardize the time variable.\n\n\n\n\n\n\nNote\n\n\n\nDate parsing is one of the most common sources of errors in data analysis. If your merge produces lots of missing values, date formats are often the reason.",
    "crumbs": [
      "Tutorials",
      "3. Data pre-processing and visualization"
    ]
  },
  {
    "objectID": "Tutorials/Data-preprocessing.html#pre-processing-and-merging",
    "href": "Tutorials/Data-preprocessing.html#pre-processing-and-merging",
    "title": "3. Data pre-processing and visualization",
    "section": "Pre-processing and merging",
    "text": "Pre-processing and merging\nFirst, ensure mobility\\(date is a real Date object\nIf mobility\\)date is already a Date, this will keep it as Date. If it’s a character like “2020-04-01”, this will parse it correctly.\n\nmobility &lt;- mobility |&gt;\n  mutate(date = as.Date(date))\n\nCreate a calendar date in covidcases based on the week number. Week 1 starts on 2019-12-29\n\ncovidcases &lt;- covidcases |&gt;\n  mutate(date = as.Date(\"2019-12-29\") + weeks(week - 1))\n\n\nMerge datasets by state and date\n\nmerged_data &lt;- mobility |&gt;\n  left_join(covidcases, by = c(\"state\", \"date\"))\nglimpse(merged_data)\n\nRows: 77,518\nColumns: 9\nGroups: state [51]\n$ state         &lt;chr&gt; \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"…\n$ date          &lt;date&gt; 2020-03-01, 2020-03-02, 2020-03-03, 2020-03-04, 2020-03…\n$ samples       &lt;int&gt; 267652, 287264, 292018, 298704, 288218, 282982, 282326, …\n$ m50           &lt;dbl&gt; 10.871941, 14.345132, 14.244603, 13.083015, 14.815029, 1…\n$ m50_index     &lt;dbl&gt; 76.92647, 98.57353, 98.25000, 89.69118, 102.38235, 126.2…\n$ county        &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, \"Shelby\", \"Baldwin\", \"Lee\", …\n$ week          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 11, 11, 11, 11, 11, 11, 11, …\n$ weekly_cases  &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, 4, 1, 3, 1, 21, 2, 3, 1, 1, …\n$ weekly_deaths &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlternatively, you can also use right_join(), innter_join(), full_join()… etc: for more, see: https://alicepaul.github.io/health-data-science-using-r/book/merging_data.html",
    "crumbs": [
      "Tutorials",
      "3. Data pre-processing and visualization"
    ]
  },
  {
    "objectID": "Tutorials/Data-preprocessing.html#line-graph-trends-over-time",
    "href": "Tutorials/Data-preprocessing.html#line-graph-trends-over-time",
    "title": "3. Data pre-processing and visualization",
    "section": "Line graph: trends over time",
    "text": "Line graph: trends over time\nYou can add a title and label the x- and y-axes to make the plot more informative. Of note, m50 represents the median of the max-distance mobility (the distance a typical member of a given population moves in a day).\n\nm50trend &lt;- ggplot(filtered_data, \n                   aes(x = week, y = m50, color = state)) +\n  geom_line() +\n  labs(\n    title = \"Weekly Mobility Trends Across Three States\",\n    x = \"Week number\",\n    y = \"m50 (Median max-distance mobility)\"\n  )\nprint(m50trend)",
    "crumbs": [
      "Tutorials",
      "3. Data pre-processing and visualization"
    ]
  },
  {
    "objectID": "Tutorials/Data-preprocessing.html#saving-plots",
    "href": "Tutorials/Data-preprocessing.html#saving-plots",
    "title": "3. Data pre-processing and visualization",
    "section": "Saving plots",
    "text": "Saving plots\nBy default, ggsave() saves figures at 7 × 7 inches. You can change the width and height to customize the figure size.\n\nggsave(\"mobility_line_graph.png\", plot = m50trend)\n\nggsave(\n  \"mobility_line_graph_large.png\",\n  plot = m50trend,\n  width = 10, height = 8, units = \"in\",\n  dpi = 300\n)\n\n\n\n\n\n\n\nTip\n\n\n\nIf ggsave() does not save where you expect, check your working directory with getwd().",
    "crumbs": [
      "Tutorials",
      "3. Data pre-processing and visualization"
    ]
  },
  {
    "objectID": "Tutorials/Data-preprocessing.html#customizing-plot-appearance",
    "href": "Tutorials/Data-preprocessing.html#customizing-plot-appearance",
    "title": "3. Data pre-processing and visualization",
    "section": "Customizing plot appearance",
    "text": "Customizing plot appearance\nYou can change the size of texts, colors/thickness/types of the lines etc by using different functions.\n\n1. Change the size of texts (increase text size for instane)\n\nm50trend_1 &lt;- ggplot(filtered_data, \n                     aes(x = week, y = m50, color = state)) +\n  geom_line() +\n  labs(title = \"Weekly Mobility Trends Across Three States\",\n       x = \"Week Number\",\n       y = \"m50 (Median Max-Distance Mobility)\") +\n  theme_minimal() +\n  theme(\n    # Bigger title\n    plot.title = element_text(size = 16, face = \"bold\"),  \n    # X-axis label size\n    axis.title.x = element_text(size = 14),  \n    # Y-axis label size\n    axis.title.y = element_text(size = 14),  \n    # Legend text size\n    legend.text = element_text(size = 12),  \n    # Legend title size\n    legend.title = element_text(size = 14)  \n  )\nprint(m50trend_1)\n\n\n\n\n\n\n\n\n\n\n2. Change line thickness\nYou can adjust the number after size = to control how thick the line appears in the plot.\n\nm50trend_2 &lt;- ggplot(filtered_data, \n                     aes(x = week, y = m50, color = state)) +\n  geom_line(size = 1.5) +  # Increase line thickness\n  labs(title = \"Weekly Mobility Trends Across Three States\",\n       x = \"Week Number\",\n       y = \"m50 (Median Max-Distance Mobility)\") +\n  theme_minimal()\nprint(m50trend_2)\n\n\n\n\n\n\n\n\n\n\n3. Change Line Type\nIf the figure will be printed in black and white, use different line types such as solid, dashed, or dotted to make each line easier to distinguish.\n\nm50trend_3 &lt;- ggplot(filtered_data, \n                     aes(x = week, y = m50, color = state, linetype = state)) +\n  geom_line(size = 1.2) +  # Thicker lines\n  labs(title = \"Weekly Mobility Trends Across Three States\",\n       x = \"Week Number\",\n       y = \"m50 (Median Max-Distance Mobility)\") +\n  theme_minimal()\nprint(m50trend_3)\n\n\n\n\n\n\n\n\n\n\n4. Assign Colors\nYou can use the scale_color_manual() function and enter the colors of your choice to customize the plot.\n\nm50trend_4 &lt;- ggplot(filtered_data, \n                     aes(x = week, y = m50, color = state)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"California\" = \"blue\", \n                                \"Florida\" = \"green\", \n                                \"Hawaii\" = \"pink\")) + \n  labs(title = \"Weekly Mobility Trends Across Three States\",\n       x = \"Week Number\",\n       y = \"m50 (Median Max-Distance Mobility)\") +\n  theme_minimal()\nprint(m50trend_4)",
    "crumbs": [
      "Tutorials",
      "3. Data pre-processing and visualization"
    ]
  },
  {
    "objectID": "Tutorials/Data-preprocessing.html#summary",
    "href": "Tutorials/Data-preprocessing.html#summary",
    "title": "3. Data pre-processing and visualization",
    "section": "Summary:",
    "text": "Summary:\nIn this chapter, you practiced inspecting datasets, converting variables, and preparing data for analysis by aligning date formats and merging datasets.\nYou also created visualizations using ggplot2 to examine trends and compare groups. These steps (cleaning, organizing, and visualizing data) are essential for turning raw data into meaningful insights.\nIn the next chapter, we will build on these skills by moving from data preparation to data analysis and interpretation.",
    "crumbs": [
      "Tutorials",
      "3. Data pre-processing and visualization"
    ]
  }
]